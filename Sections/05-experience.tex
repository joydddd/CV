%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Experience  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \SubHeadingListStart
     \SubHeading
        {PyTorch group, Meta Inc.}{2024,25}
            % POSITION 1
            {Research Scientist Intern}{}
            \ItemListStart
                % \Item{Project: \underline{\href{https://pytorch.org/blog/flexattention/}{FlexAttention}: The Flexibility of PyTorch with the Performance of FlashAttention}}

                \Item{Contribute to TorchInductor \& Helion DSL. }
                \Item{Develop new techniques in PyTorch compiler with a focus on GPU performance optimization. }      
                \Item{Design GPU programming language for fast, flexible, and easy-to-use ML kernel authoring. }
                \Item{Research new techniques for high-performance distributed GPU communication. } 
                \Item{Engage in the open source community to identify user needs and promote new features. }
            \ItemListEnd

        % COMPANY 1
        \SubHeading
        {NVIDIA}{2022, 2025}
            % POSITION 1
            {Deep Learning Architect Intern}{}
            \ItemListStart
                \Item{Model and analyze new memory features on next-gen GPUs such as distributed shared memory, asynchronous transaction barrier, etc.}
                \Item{Analyze and optimize multi-GPU data movement for deep learning workloads using Tensor Memory Accelerator (TMA).}
                \Item{Design and improve compiler abstractions and programming interface for GPU domain-specific languages. }
            \ItemListEnd
            
    \SubHeadingListEnd