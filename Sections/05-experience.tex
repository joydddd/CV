%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Experience  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \SubHeadingListStart
     \SubHeading
        {PyTorch group, Meta Inc.}{2024-25}
            % POSITION 1
            {Rearch Scientist Intern}{}
            \ItemListStart
                % \Item{Project: \underline{\href{https://pytorch.org/blog/flexattention/}{FlexAttention}: The Flexibility of PyTorch with the Performance of FlashAttention}}

                \Item{Contribute to TorchDynamo, TorchInductor \& TorchDistributed. }
                \Item{Develop new techniques in PyTorch compiler with a focus on GPU performance optimization. }      
                \Item{Design GPU programming language for fast, flexible, and easy-to-use ML kernel authoring. }
                \Item{Research new techniques for high-performance distributed GPU communication. } 
                \Item{Engage in the open source community to identify user needs and promote new features. }
            \ItemListEnd

        % COMPANY 1
        \SubHeading
        {NVIDIA}{2022}
            % POSITION 1
            {GPU Deep Learning Architect Intern}{}
            \ItemListStart
                \Item{Model and analyze new memory features on next-gen GPUs such as distributed shared memory, asynchronous transaction barrier, etc.}
                \Item{Analyze and optimize multi-GPU data movement for deep learning workloads using Tensor Memory Accelerator (TMA).}
            \ItemListEnd
            
    \SubHeadingListEnd